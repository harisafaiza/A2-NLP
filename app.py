# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B3DB5-iQKW1LNu0TAjVhvFIBM02_uwcr
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer
import streamlit as st
import torch.nn.functional as F

# Define the LSTM Model
class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(LSTMModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        # Ensure input tensor indices are within the valid range
        if x.max() >= self.embedding.num_embeddings:
            raise ValueError(f"Index {x.max()} is out of range for vocab size {self.embedding.num_embeddings}.")

        x = self.embedding(x)
        x, _ = self.lstm(x)
        x = self.fc(x[:, -1, :])  # Take the last output from the LSTM
        return x

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Check the vocabulary size
vocab_size = len(tokenizer)
embedding_dim = 10
hidden_dim = 50

# Initialize the LSTM model
lstm_model = LSTMModel(vocab_size, embedding_dim, hidden_dim)

# Load pre-trained model (if available)
try:
    lstm_model.load_state_dict(torch.load("model/lstm_model.pt"))
    lstm_model.eval()
    print("Model loaded successfully.")
except Exception as e:
    print(f"Model loading failed: {e}")

# Streamlit App
def app():
    st.title("LSTM Model Inference")

    # Input text
    input_text = st.text_area("Enter Text:", "This is a sample input text.")

    # Tokenize and convert text to tensor
    input_ids = tokenizer.encode(input_text, return_tensors="pt")

    # Check if the input indices are within the vocabulary size
    if input_ids.max() >= vocab_size:
        st.error(f"Error: Index {input_ids.max()} is out of range for vocab size {vocab_size}.")
        return

    # Pass the input through the LSTM model
    try:
        input_tensor = torch.tensor(input_ids)
        output = lstm_model(input_tensor)

        # Display the raw output
        st.write(f"Raw Model Output: {output.item()}")

        # Apply sigmoid if it's a binary classification model
        # output_prob = torch.sigmoid(output).item()
        # st.write(f"Model Output (Probability): {output_prob:.4f}")

        # If it's a regression task, you might directly display the value
        st.write(f"Model Output (Raw Value): {output.item():.4f}")
    except Exception as e:
        st.error(f"Error during inference: {e}")

# Run the Streamlit app
if __name__ == "__main__":
    app()
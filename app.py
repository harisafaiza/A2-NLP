# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B3DB5-iQKW1LNu0TAjVhvFIBM02_uwcr
"""

import os
import torch
import streamlit as st
from torch import nn
from torch.optim import Adam

# Streamlit UI for model loading and inference
st.title("LSTM Model Inference")

# Create 'model' directory if it doesn't exist
if not os.path.exists("model"):
    os.makedirs("model")
    st.write("Created 'model' directory.")
else:
    st.write("'model' directory already exists.")

# Define your model architecture (e.g., LSTM)
class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size):
        super(LSTMModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x):
        embedded = self.embedding(x)
        output, (hidden, cell) = self.lstm(embedded)
        output = self.fc(output[:, -1, :])  # Take the last time step output
        return output

# Load model if it exists
model_path = "model/lstm_model.pt"
if os.path.exists(model_path):
    st.write(f"Loading model from {model_path}...")
    lstm_model = torch.load(model_path)
    lstm_model.eval()  # Set model to evaluation mode
    st.write("Model loaded successfully!")
else:
    st.write(f"Model not found at {model_path}. Please ensure the model is saved first.")

# Inference Example
if st.button('Run Inference'):
    if 'lstm_model' in locals():
        # Example input data for inference (replace with actual input)
        input_data = torch.tensor([1, 2, 3])  # Example: Replace with actual tokenized input
        output = lstm_model(input_data)
        st.write("Model Output: ", output)
    else:
        st.write("Model is not loaded.")

# Add model saving functionality for testing
if st.button('Save Model'):
    # Define a new model to save
    vocab_size = 5000  # Example vocab size
    embed_size = 10
    hidden_size = 50
    model_to_save = LSTMModel(vocab_size, embed_size, hidden_size)

    # Save model
    torch.save(model_to_save, model_path)
    st.write(f"Model saved at {model_path}")